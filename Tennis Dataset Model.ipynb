{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a5991e",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a370093",
   "metadata": {},
   "source": [
    "Given that we have a large dataset and a mix of categorical and numerical data, we can start with Random Forest due to its ability to handle complex relationships and mixed data types without extensive preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    log_loss\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('path_to_your_dataset.csv')\n",
    "\n",
    "# Preprocessing steps (assuming they have been done)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('player_victory', axis=1)\n",
    "y = df['player_victory']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Log Loss: {logloss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0a8a8",
   "metadata": {},
   "source": [
    "To evaluate our classification model comprehensively, we can use a variety of metrics. Each metric provides different insights into the performance of the model. Here are some common metrics used for binary classification tasks which we have printed out for this model:\n",
    "\n",
    "Accuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
    "\n",
    "Precision: The ratio of true positives to all positive predictions. Precision is a measure of the accuracy of the positive predictions.\n",
    "\n",
    "Recall (Sensitivity): The ratio of true positives to all actual positives. Recall measures the ability of the classifier to find all the positive samples.\n",
    "\n",
    "F1 Score: The harmonic mean of precision and recall. An F1 score balances the trade-off between precision and recall.\n",
    "\n",
    "ROC-AUC Score: The area under the receiver operating characteristic (ROC) curve. It is a plot of the true positive rate against the false positive rate for the different possible cut points of a diagnostic test.\n",
    "\n",
    "Confusion Matrix: A table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "Log Loss (Cross-Entropy Loss): Measures the performance of a classification model where the prediction is a probability between 0 and 1. The loss increases as the predicted probability diverges from the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5afe60",
   "metadata": {},
   "source": [
    "## Results Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf288d",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred are already defined as they are the true labels and model predictions respectively\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdbf95",
   "metadata": {},
   "source": [
    "This is a table that is used to describe the performance of a classification model.\n",
    "\n",
    "True Positives (TP): The cases in which the model correctly predicted the positive class.\n",
    "True Negatives (TN): The cases in which the model correctly predicted the negative class.\n",
    "False Positives (FP): The cases in which the model incorrectly predicted the positive class (also known as a \"Type I error\").\n",
    "False Negatives (FN): The cases in which the model incorrectly predicted the negative class (also known as a \"Type II error\").\n",
    "\n",
    "The heatmap visualization of the confusion matrix uses color to emphasize the different values, with darker colors typically representing higher numbers. This visualization makes it easy to see the proportion of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eec7b4",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb11604",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "True Positive Rate (TPR): Also known as recall, it is the ratio of TP to the sum of TP and FN.\n",
    "False Positive Rate (FPR): It is the ratio of FP to the sum of FP and TN.\n",
    "The area under the ROC curve (AUC) is a measure of the model's ability to distinguish between the classes. An AUC of 0.5 suggests no discrimination (i.e., random chance), while an AUC of 1.0 indicates perfect discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e903f",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f63d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6683d",
   "metadata": {},
   "source": [
    "The Precision-Recall curve shows the trade-off between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\n",
    "\n",
    "Precision: The ratio of TP to the sum of TP and FP.\n",
    "Recall: The ratio of TP to the sum of TP and FN.\n",
    "This curve is particularly useful when the classes are very imbalanced. Unlike the ROC curve, the Precision-Recall curve focuses on the performance of the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7adc83",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_classifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c16fc5",
   "metadata": {},
   "source": [
    "Feature Importance\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable. Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Random Forest Classifier for extracting the top features for our dataset.\n",
    "\n",
    "Higher Bar: Indicates that the feature is more important for the model when making predictions.\n",
    "Lower Bar: Indicates that the feature is less important.\n",
    "In the bar chart, each bar represents a feature in the dataset, and the length of the bar corresponds to the importance score. This helps in understanding which features have the most impact on the predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5520c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlhw",
   "language": "python",
   "name": "mlhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
